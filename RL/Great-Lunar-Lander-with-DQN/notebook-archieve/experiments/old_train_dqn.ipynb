{"cells":[{"cell_type":"markdown","metadata":{"id":"OchQ2AhXW87r"},"source":["# Deep Q-Network\n","\n","Reference: [Landing a Rocket with Reinforcement Learning](https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055)\n","\n","```\n","initialize replay memory R\n","initialize action-value function Q (with random weights)\n","observe initial state s\n","repeat\n","\tselect an action a\n","\t\twith probability ϵ select a random action\n","\t\totherwise select a= argmaxa′Q(s,a′)\n","\tcarry out action a\n","\tobserve reward rr and new state s’\n","\tstore experience <s,a,r,s> in replay memory R\n","\tsample random transitions <ss,aa,rr,ss′>from replay memory R\n","\tcalculate target for each minibatch transition\n","\t\tif ss’ is terminal state then tt =rr otherwise tt =rr + γmaxa′Q(ss′,aa′)\n","\ttrain the Q network using (tt−Q(ss,aa))2 as loss\n","\ts=s′\n","until terminated\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ye8BEdjDCaQH"},"outputs":[],"source":["%%capture\n","!pip install Box2D\n","!pip install box2d\n","!pip install box2d-py\n","!pip install gym[all]\n","!pip install gym[Box_2D]\n","!pip install wandb\n","!pip install knockknock"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2632,"status":"ok","timestamp":1643068043987,"user":{"displayName":"Muhammad Faqih Akmal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQ0Gv9uTEIjqztfgFQ3ZRDb-p9ZzwH_0sqIhp02Q=s64","userId":"03909925437233836922"},"user_tz":-480},"id":"vWn0mg_gYx38","outputId":"a5b1cb80-944d-4ee5-e83f-e95e34e97ebe"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/Othercomputers/razer13/0-assignments/ca2/part-b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9n-y4eM9-7r"},"outputs":[],"source":["import gym\n","from knockknock import telegram_sender\n","\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","\n","from utils import seed_everything, Experience, ReplayBuffer\n","from collections import deque\n","from model.dqn import DQN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":5657,"status":"ok","timestamp":1643068052291,"user":{"displayName":"Muhammad Faqih Akmal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQ0Gv9uTEIjqztfgFQ3ZRDb-p9ZzwH_0sqIhp02Q=s64","userId":"03909925437233836922"},"user_tz":-480},"id":"cKD5Yr0e-H2d","outputId":"9a814089-fe21-491f-8979-56ec95dd8f94"},"outputs":[],"source":["import wandb\n","wandb.login()\n","wandb.init(\n","    entity=\"onsen\",\n","    project=\"dqn-lunar-lander\"\n",")\n","# 8b99e70d604baf9855037e8ebf97bacb8af829fc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsTQtE5UAKNH"},"outputs":[],"source":["# Set random seed to 1\n","seed_everything(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"sLJrHkccAlDN","outputId":"228b4862-8878-4b9c-9e86-2e8b06070a33"},"outputs":[],"source":["env = gym.make('LunarLander-v2')\n","# env = gym.make('CartPole-v1')\n","\n","# set seeds\n","env.seed(1)\n","\n","# setting up params\n","lr = 0.001\n","epsilon = 1.0\n","epsilon_decay = 0.995\n","gamma = 0.99\n","training_episodes = 2000\n","noisy = True\n","\n","# create new deep q-network instance\n","model = DQN(env=env, lr=lr, gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay, log_wandb=True, noisy = noisy)\n","\n","# training\n","@telegram_sender(token=\"5158827103:AAE-QThCKlqgqBwRgR_heJjqB1BarWjdIxk\", chat_id=943489922)\n","def train_model(model):\n","    model.train(training_episodes, mean_stopping=True)\n","\n","train_model(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["27fd555c9fd04a458b6fee70712c30e8"]},"id":"NYwSGPTTREui","outputId":"da272b3b-c6f1-4b59-d254-1b5be076716e"},"outputs":[],"source":["# Complete training\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"p2hegubst1hB"},"outputs":[],"source":["model.save('./saved-models/dqn/toasty_plant_44.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9O1FUY3LHlqE"},"outputs":[],"source":["def plot_df(df, chart_name, title, x_axis_label, y_axis_label):\n","    plt.rcParams.update({'font.size': 17})\n","    df['rolling_mean'] = df[df.columns[0]].rolling(100).mean()\n","    plt.figure(figsize=(15, 8))\n","    plt.close()\n","    plt.figure()\n","    # plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n","    plot = df.plot(linewidth=1.5, figsize=(15, 8))\n","    plot.set_xlabel(x_axis_label)\n","    plot.set_ylabel(y_axis_label)\n","    # plt.ylim((-400, 300))\n","    fig = plot.get_figure()\n","    plt.legend().set_visible(False)\n","    fig.savefig(chart_name)\n","\n","def plot_df2(df, chart_name, title, x_axis_label, y_axis_label):\n","    df['mean'] = df[df.columns[0]].mean()\n","    plt.rcParams.update({'font.size': 17})\n","    plt.figure(figsize=(15, 8))\n","    plt.close()\n","    plt.figure()\n","    # plot = df.plot(linewidth=1.5, figsize=(15, 8), title=title)\n","    plot = df.plot(linewidth=1.5, figsize=(15, 8))\n","    plot.set_xlabel(x_axis_label)\n","    plot.set_ylabel(y_axis_label)\n","    plt.ylim((0, 300))\n","    plt.xlim((0, 100))\n","    plt.legend().set_visible(False)\n","    fig = plot.get_figure()\n","    fig.savefig(chart_name)\n","\n","def plot_experiments(df, chart_name, title, x_axis_label, y_axis_label, y_limit):\n","    plt.rcParams.update({'font.size': 17})\n","    plt.figure(figsize=(15, 8))\n","    plt.close()\n","    plt.figure()\n","    plot = df.plot(linewidth=1, figsize=(15, 8), title=title)\n","    plot.set_xlabel(x_axis_label)\n","    plot.set_ylabel(y_axis_label)\n","    plt.ylim(y_limit)\n","    fig = plot.get_figure()\n","    fig.savefig(chart_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l3q9Vi_3IIad"},"outputs":[],"source":["# test the model\n","trained_model = load_model(save_dir + \"trained_model_new.h5\")\n","test_rewards = test_already_trained_model(trained_model)\n","pickle.dump(test_rewards, open(save_dir + \"test_rewards_new.p\", \"wb\"))\n","test_rewards = pickle.load(open(save_dir + \"test_rewards_new.p\", \"rb\"))\n","\n","plot_df2(\n","    pd.DataFrame(test_rewards), \n","    \"Figure 2: Reward for each testing episode\", \n","    \"Reward for each testing episode\", \n","    \"Episode\", \n","    \"Reward\"\n",")\n","\n","print(\"Training and Testing Completed...!\")"]},{"cell_type":"markdown","metadata":{"id":"JCVydQlcHvsi"},"source":["# Hyperparameter Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91hX1Be5unJQ"},"outputs":[],"source":["def run_experiment_for_gamma():\n","    print('Running Experiment for gamma...')\n","    env = gym.make('LunarLander-v2')\n","\n","    # set seeds\n","    env.seed(21)\n","    np.random.seed(21)\n","\n","    # setting up params\n","    lr = 0.001\n","    epsilon = 1.0\n","    epsilon_decay = 0.995\n","    gamma_list = [0.99, 0.9, 0.8, 0.7]\n","    training_episodes = 1000\n","\n","    rewards_list_for_gammas = []\n","    \n","    for gamma_value in gamma_list:\n","        # save_dir = \"hp_gamma_\"+ str(gamma_value) + \"_\"\n","        model = DQN(env, lr, gamma_value, epsilon, epsilon_decay)\n","        print(\"Training model for Gamma: {}\".format(gamma_value))\n","        model.train(training_episodes, False)\n","        rewards_list_for_gammas.append(model.rewards_list)\n","\n","    pickle.dump(rewards_list_for_gammas, open(\"rewards_list_for_gammas.p\", \"wb\"))\n","    rewards_list_for_gammas = pickle.load(open(\"rewards_list_for_gammas.p\", \"rb\"))\n","\n","    gamma_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n","    \n","    for i in range(len(gamma_list)):\n","        col_name = \"gamma=\" + str(gamma_list[i])\n","        gamma_rewards_pd[col_name] = rewards_list_for_gammas[i]\n","    \n","    plot_experiments(\n","        gamma_rewards_pd,\n","        \"Figure 4: Rewards per episode for different gamma values\",\n","        \"Figure 4: Rewards per episode for different gamma values\",\n","        \"Episodes\",\n","        \"Reward\",\n","        (-600, 300)\n","    )\n","\n","def run_experiment_for_lr():\n","    print('Running Experiment for learning rate...')\n","    env = gym.make('LunarLander-v2')\n","\n","    # set seeds\n","    env.seed(21)\n","    np.random.seed(21)\n","\n","    # setting up params\n","    lr_values = [0.0001, 0.001, 0.01, 0.1]\n","    epsilon = 1.0\n","    epsilon_decay = 0.995\n","    gamma = 0.99\n","    training_episodes = 1000\n","    rewards_list_for_lrs = []\n","    \n","    for lr_value in lr_values:\n","        model = DQN(env, lr_value, gamma, epsilon, epsilon_decay)\n","        print(\"Training model for LR: {}\".format(lr_value))\n","        model.train(training_episodes, False)\n","        rewards_list_for_lrs.append(model.rewards_list)\n","\n","    pickle.dump(rewards_list_for_lrs, open(\"rewards_list_for_lrs.p\", \"wb\"))\n","    rewards_list_for_lrs = pickle.load(open(\"rewards_list_for_lrs.p\", \"rb\"))\n","\n","    lr_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes + 1)))\n","    \n","    for i in range(len(lr_values)):\n","        col_name = \"lr=\"+ str(lr_values[i])\n","        lr_rewards_pd[col_name] = rewards_list_for_lrs[i]\n","    \n","    plot_experiments(\n","        lr_rewards_pd,\n","        \"Figure 3: Rewards per episode for different learning rates\",\n","        \"Figure 3: Rewards per episode for different learning rates\",\n","        \"Episodes\", \n","        \"Reward\",\n","        (-2000, 300)\n","    )\n","\n","def run_experiment_for_ed():\n","    print('Running Experiment for epsilon decay...')\n","    env = gym.make('LunarLander-v2')\n","\n","    # set seeds\n","    env.seed(21)\n","    np.random.seed(21)\n","\n","    # setting up params\n","    lr = 0.001\n","    epsilon = 1.0\n","    ed_values = [0.999, 0.995, 0.990, 0.9]\n","    gamma = 0.99\n","    training_episodes = 1000\n","\n","    rewards_list_for_ed = []\n","    \n","    for ed in ed_values:\n","        save_dir = \"hp_ed_\"+ str(ed) + \"_\"\n","        model = DQN(env, lr, gamma, epsilon, ed)\n","        print(\"Training model for ED: {}\".format(ed))\n","        model.train(training_episodes, False)\n","        rewards_list_for_ed.append(model.rewards_list)\n","\n","    pickle.dump(rewards_list_for_ed, open(\"rewards_list_for_ed.p\", \"wb\"))\n","    rewards_list_for_ed = pickle.load(open(\"rewards_list_for_ed.p\", \"rb\"))\n","\n","    ed_rewards_pd = pd.DataFrame(index=pd.Series(range(1, training_episodes+1)))\n","    \n","    for i in range(len(ed_values)):\n","        col_name = \"epsilon_decay = \"+ str(ed_values[i])\n","        ed_rewards_pd[col_name] = rewards_list_for_ed[i]\n","    \n","    plot_experiments(\n","        ed_rewards_pd,\n","        \"Figure 5: Rewards per episode for different epsilon(ε) decay\",\n","        \"Figure 5: Rewards per episode for different epsilon(ε) decay values\",\n","        \"Episodes\",\n","        \"Reward\",\n","        (-600, 300)\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqh0uXZDILEx"},"outputs":[],"source":["# Run experiments for hyper-parameter\n","run_experiment_for_lr()\n","run_experiment_for_ed()\n","run_experiment_for_gamma()"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"train_dqn.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
