{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/kiritowu/Great-Lunar-Lander/blob/main/Setting_Up_RL_Hyperparameter_Sweeps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"rs9l4nQyeZ1N"},"source":["# Hyperparameter Tuning of RL Models\n","\n","The purpose of this notebook is to set up hyperparameter tuning using Weights and Biases. Using W&B lets us coordinate a hyperparameter search across several machines, speeding up the learning process.\n"]},{"cell_type":"markdown","metadata":{"id":"T1ubLhlDez8R"},"source":["## Setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"In-nEwIvdF23"},"outputs":[],"source":["%%capture\n","!pip install wandb --upgrade # Don't run if not on Google Colab unless you don't have W&B installed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"id":"ZtLjVdMEex0X","outputId":"77cff604-a110-48f9-a5f7-1473abd28143"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","\n","wandb.login()\n"]},{"cell_type":"markdown","metadata":{"id":"bBsYgCQqe3We"},"source":["## Define Sweep\n"]},{"cell_type":"markdown","metadata":{"id":"8fOdB3kxe9zi"},"source":["### Selecting a Search Method\n","\n","There are three methods available:\n","\n","- Grid Search\n","- Random Search\n","- Bayesian Search\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2NJIOJLfAR0"},"outputs":[],"source":["sweep_config = {\n","    \"method\": \"random\",\n","    \"metric\" : {\n","        \"name\" : \"Avg-Reward-100e\",\n","        \"goal\" : \"maximize\",\n","        \"target\" : 300\n","        }\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"A5ktrkAOfIVN"},"source":["### Selecting Hyperparameters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtK9-3V6fK2G"},"outputs":[],"source":["parameters_ddqn = {\n","    \"lr\" : {\n","        \"min\" : 0.0001,\n","        \"max\" : 0.01,\n","        \"distribution\" : \"uniform\"\n","    },\n","    \"gamma\" : {\n","        \"value\" : 0.99\n","    },\n","    \"epsilon\" : {\n","        \"value\" : 1.0\n","    },\n","    \"epsilon_decay\" : {\n","        \"min\" : 0.95,\n","        \"max\" : 0.995,\n","        \"distribution\" : \"uniform\"\n","    },\n","    \"update_target_net_interval\" : {\n","        \"values\" : [1, 5, 10, 20, 30, 50, 100]\n","    },\n","    \"episodes\" : {\n","        \"value\" : 500\n","    }\n","}\n","\n","parameters_sarsa = {\n","    \"lr\" : {\n","        \"min\" : 0.0001,\n","        \"max\" : 0.01,\n","        \"distribution\" : \"uniform\"\n","    },\n","    \"gamma\" : {\n","        \"value\" : 0.99\n","    },\n","    \"epsilon\" : {\n","        \"value\" : 1.0\n","    },\n","    \"epsilon_decay\" : {\n","        \"min\" : 0.95,\n","        \"max\" : 0.995,\n","        \"distribution\" : \"uniform\"\n","    },\n","    \"episodes\" : {\n","        \"value\" : 500\n","    }\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUBdg26EL2UY"},"outputs":[],"source":["sweep_config[\"parameters\"] = parameters_sarsa"]},{"cell_type":"markdown","metadata":{"id":"e1Bq9DCRfZwj"},"source":["# Initialize Sweep\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2txALsvofs7E"},"outputs":[],"source":["project_name = \"SARSA-Tuning\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZ6qmewYfgOF","outputId":"e7c983b5-7f6a-4824-af70-6bceddf4c91e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: f1zo1g3l\n","Sweep URL: https://wandb.ai/onsen/SARSA-Tuning/sweeps/f1zo1g3l\n"]}],"source":["sweep_id = wandb.sweep(sweep_config, project=project_name, entity=\"onsen\")\n"]},{"cell_type":"markdown","metadata":{"id":"4qZz2RHgfzFI"},"source":["# Next Steps\n","\n","Now that the sweep has been initialized, we need to do the following:\n","\n","1. Define a train function for the model, that does the following:\n","\n","- Accepts a single argument `config`\n","- Initializes a new run, `wandb.init(config=config)`\n","- Builds the model with the selected hyperparameters\n","- Trains the model and logs the performance of the model.\n","\n","An example is shown below:\n","\n","```python\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","def train(config=None):\n","    # Initialize a new wandb run\n","    with wandb.init(config=config):\n","        # If called by wandb.agent, as below,\n","        # this config will be set by Sweep Controller\n","        config = wandb.config\n","\n","        loader = build_dataset(config.batch_size)\n","        network = build_network(config.fc_layer_size, config.dropout)\n","        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n","\n","        for epoch in range(config.epochs):\n","            avg_loss = train_epoch(network, loader, optimizer)\n","            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n","```\n","\n","2. On each machine you want to train, run the following:\n","\n","```python\n","wandb.agent(sweep_id, train_function, count = num_runs)\n","```\n","\n","`count` is the number of runs to perform. If not specified, search will perform forever unless grid search is used.\n"]}],"metadata":{"colab":{"name":"setup_sweep.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}